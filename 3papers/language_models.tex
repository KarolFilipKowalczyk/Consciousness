\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{mathtools}
\usepackage{geometry}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{algorithm}
\usepackage{algpseudocode}

\geometry{margin=1in}

% Theorem environments
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{prediction}[theorem]{Prediction}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{limitation}[theorem]{Limitation}

\title{Language Models as Hierarchical Computational Projections:\\A Theoretical Framework with Empirical Predictions}
\author{Karol Kowalczyk}
\date{November 9, 2025}

\begin{document}

\maketitle

\begin{abstract}
Large language models (LLMs) exhibit strikingly regular scaling patterns linking computational resources to capability, yet existing theories fail to explain why qualitative changes in reasoning arise only beyond certain model sizes. This paper situates LLMs within a formal hierarchy of finite computational systems, extending the \emph{Adjoint Projections on Computational Hierarchies} framework. We propose that each model functions as a finite machine $L_n = (S_n, f_n, \pi_n)$ at hierarchy level $n_L = \lfloor\alpha \log_2 P + \beta \log_2 V + \gamma\rfloor$, where $P$ is parameter count and $V$ is vocabulary size. Distillation and fine-tuning instantiate the adjunction's projection ($P$) and collapse ($C$) operators, governing information compression and expansion. We derive testable predictions: (1) distillation energy should scale as $E \propto k_B T \Delta H$ plus architecture-dependent overhead, (2) emergent abilities should cluster near critical levels $n \approx 33$--35, and (3) behavioral distance should predict performance degradation under compression. We outline experimental protocols to validate these predictions across model families and discuss implications for scaling law theory, computational efficiency, and the theoretical foundations of artificial intelligence.
\end{abstract}

\noindent\textbf{Status:} This paper presents a theoretical framework with proposed empirical validation. Experimental results are outlined as predictions to be tested.

\section{Introduction}

\subsection{Scaling and the puzzle of emergence}

Large language model development follows empirical scaling laws where performance improves according to power relationships with model size. Kaplan et al.\ (2020) showed that cross-entropy loss $L$ scales approximately as $L \propto N^{-\alpha}$ where $N$ represents parameters or training tokens, with $\alpha \approx 0.05$--0.1. Hoffmann et al.\ (2022) refined these relationships through compute-optimal training schedules.

However, smooth scaling curves conceal a deeper phenomenon. Wei et al.\ (2022) and Ganguli et al.\ (2022) documented that new capabilities---multi-step reasoning, mathematical problem-solving, self-consistent planning---emerge discontinuously. A 1.3B-parameter model may completely fail tasks that a 6.7B-parameter model solves reliably. These \emph{emergent abilities} suggest structural reorganizations analogous to phase transitions in physical systems.

\textbf{The fundamental question:} What determines when and why these discontinuous transitions occur? Standard scaling laws offer no answer. We need a structural theory of computation accounting for how representational capacity and information flow scale with model complexity.

\subsection{Hierarchical computation as a unifying principle}

We adopt a hierarchical perspective building on the \emph{Adjoint Projections on Computational Hierarchies} framework (Kowalczyk, 2025). Computation is modeled as a nested sequence of finite machines $\{M_n\}$, each operating on state spaces of size $2^n$. Higher levels simulate lower ones through embeddings, while projections compress state spaces. The adjunction $C \dashv P$ expresses duality between expansion (collapse $C$) and compression (projection $P$).

\textbf{Key insight:} When mapped to LLMs, these abstract operations correspond naturally to concrete practices:
\begin{itemize}
\item \textbf{Projection $\approx$ Distillation:} Compressing a large teacher model into a smaller student
\item \textbf{Collapse $\approx$ Fine-tuning:} Expanding or enriching representations through additional training
\end{itemize}

This mapping transforms qualitative emergence into a quantitative hypothesis: emergent behavior occurs at critical points where projections between adjacent hierarchy levels become irreversibly lossy, forcing representational reorganization.

\subsection{Contributions}

This paper contributes:

\begin{enumerate}
\item \textbf{Theoretical mapping:} Explicit correspondence between LLM attributes (parameters, vocabulary) and hierarchy levels $n_L$, with derived level assignment formula

\item \textbf{Testable predictions:} Three quantitative predictions regarding energy costs, emergence thresholds, and performance degradation under compression

\item \textbf{Experimental protocols:} Detailed methodology for validating predictions across 20--30 models from diverse families

\item \textbf{Unification:} Integration of scaling laws, information theory, and thermodynamic cost into a coherent hierarchical framework
\end{enumerate}

\subsection{Limitations and scope}

\begin{limitation}[No empirical results]
This paper provides a theoretical framework with testable predictions but does not present experimental validation. The predictions remain to be tested empirically.
\end{limitation}

\begin{limitation}[Simplified model mapping]
\label{lim:simplified}
Our treatment of neural networks as finite state machines abstracts away continuous activations, gradient dynamics, and stochastic training processes. This simplification enables mathematical tractability but may miss important aspects of neural network behavior.
\end{limitation}

\begin{limitation}[Physical claims require validation]
\label{lim:physical}
The thermodynamic predictions (e.g., $E \propto k_B T \Delta H$) are derived from information-theoretic principles but GPU energy consumption involves many confounding factors (memory bandwidth, parallelization overhead, cooling requirements). The predicted relationship should be viewed as a theoretical baseline requiring empirical calibration.
\end{limitation}

\begin{limitation}[Baseline comparisons needed]
This framework should be compared systematically with existing neural scaling theories, including effective theories of overparameterized networks, neural tangent kernel analyses, and empirical scaling law models.
\end{limitation}

\subsection{Paper organization}

Section~\ref{sec:related} reviews related work on scaling, emergence, and information theory. Section~\ref{sec:framework} presents the theoretical framework connecting adjunction theory to neural networks. Section~\ref{sec:level} derives the level assignment formula and proposes validation methodology. Section~\ref{sec:distillation} analyzes distillation as projection with thermodynamic implications. Section~\ref{sec:predictions} derives scaling predictions and identifies critical transitions. Section~\ref{sec:discussion} discusses limitations and future work. Section~\ref{sec:conclusion} concludes.

\section{Related Work}
\label{sec:related}

\subsection{Scaling laws and performance predictability}

Neural network scaling laws quantify performance-resource relationships. Kaplan et al.\ (2020) and Hoffmann et al.\ (2022) showed that loss decreases predictably with compute, enabling extrapolation from small to large models. This regularity suggests underlying invariants analogous to universal scaling in statistical mechanics. However, power laws describe \emph{continuous} improvement but fail to capture \emph{discrete} capability jumps.

\subsection{Emergent abilities and discontinuous transitions}

Wei et al.\ (2022) documented abrupt capability transitions: arithmetic reasoning, logical inference, and complex planning appear suddenly at specific model sizes. Schaeffer et al.\ (2023) argued some apparent discontinuities reflect measurement artifacts, though genuine phase changes remain. Ganguli et al.\ (2022) hypothesized transitions correspond to representational phase changes when information per parameter exceeds critical thresholds. However, these analyses lack mathematical models connecting emergence to computability or information geometry. The hierarchical framework provides that missing structure.

\subsection{Compression, distillation, and knowledge transfer}

Knowledge distillation (Hinton et al., 2015) transfers knowledge from large teachers to small students by minimizing KL divergence between output distributions. This realizes projection mathematically: compressing high-dimensional representational manifolds into lower-dimensional approximations while preserving behavioral equivalence. Empirical work (Sanh et al., 2019; Jiao et al., 2020) demonstrates large efficiency gains at the cost of reduced output diversity---measurable information loss $\Delta H$.

\subsection{Information-theoretic perspectives}

The information bottleneck theory (Tishby \& Zaslavsky, 2015) conceptualizes learning as trading compression against relevance. Subsequent analyses (Saxe et al., 2019) refined this for layer-level information flow but not for transitions between distinct model families. Our framework extends the bottleneck concept to entire computational hierarchies with cross-level metrics.

\subsection{Computational complexity and descriptive hierarchy}

Theoretically, language models function as finite automata with parametric extension. Their complexity class grows with parameter count and token diversity. Tyszkiewicz (1998, 2004, 2010) analyzed analogous hierarchies in database theory using games and Kolmogorov complexity to quantify expressivity gaps. We reinterpret these results for machine learning, where expressivity gaps manifest as capability thresholds.

\textbf{Gap in existing work:} No prior theory connects smooth scaling laws, discontinuous emergence, compression costs, and information theory into a unified predictive framework. Our hierarchical approach provides this unification.

\section{Theoretical Framework}
\label{sec:framework}

\subsection{Review: Hierarchical computational machines}

The foundation comes from \emph{Adjoint Projections on Computational Hierarchies} (Kowalczyk, 2025). Each hierarchy level corresponds to a finite machine $M_n = (S_n, f_n, \pi_n)$ with:
\begin{itemize}
\item State space $S_n$ of size $|S_n| = 2^n$
\item Deterministic transition function $f_n: S_n \to S_n$
\item Stationary probability distribution $\pi_n: S_n \to [0,1]$
\end{itemize}

Information capacity is $I_n = n$ bits. Levels connect via:
\begin{itemize}
\item \textbf{Embeddings} $\sigma_{i\to j}: S_i \hookrightarrow S_j$ (injective, structure-preserving)
\item \textbf{Projections} $P_{j\to i}: S_j \twoheadrightarrow S_i$ (surjective, entropy-minimizing)
\item \textbf{Collapses} $C_{i\to j}: S_i \hookrightarrow S_j$ (injective, error-minimizing, satisfying $P\circ C = \text{id}$)
\end{itemize}

The pair $(C, P)$ forms an adjunction $C \dashv P$ with:
\begin{itemize}
\item Unit $\eta: \text{id} \Rightarrow C\circ P$
\item Counit $\varepsilon: P\circ C \Rightarrow \text{id}$
\item Triangle identities: $(\varepsilon P)\circ(P\eta) = \text{id}_P$ and $(C\varepsilon)\circ(\eta C) = \text{id}_C$
\end{itemize}

\textbf{Behavioral distance} $\text{Beh}(i,j)$ quantifies computational divergence between levels via normalized Hamming disagreement on common embedded domains. The cross-level metric $d(M_i, M_j) = |2^{-i} - 2^{-j}| + 2^{-\min(i,j)} \cdot \text{Beh}(i,j)$ defines a metric space with completion $T_c$ (the \emph{computational continuum}).

\subsection{Mapping language models to hierarchy levels}

An LLM maps onto this framework as follows:

\textbf{States $(S_n)$:} The state space comprises:
\begin{itemize}
\item Hidden representations $h \in \mathbb{R}^d$ at each layer
\item Token embeddings $e \in \mathbb{R}^{d_{\text{embed}}}$
\item Attention patterns and intermediate activations
\end{itemize}

Effective state space size scales with parameter count $P$ and embedding dimension $d$: $|S_n| \propto 2^d \cdot P$ (heuristically, since $P$ parameters define a $P$-dimensional weight space over quantized values).

\textbf{Transitions $(f_n)$:} Layer-wise transformations implement $f_n$:
\[
f_n(h) = \text{LayerNorm}(h + \text{MHA}(h) + \text{FFN}(h))
\]
where MHA is multi-head attention and FFN is feed-forward network. Each layer maps hidden states deterministically (during inference with fixed weights).

\textbf{Distributions $(\pi_n)$:} Output softmax defines $\pi_n$:
\[
\pi_n(\text{token} \mid \text{context}) = \text{softmax}(W_{\text{out}} \cdot h_{\text{final}})
\]
where $h_{\text{final}}$ is the final hidden state and $W_{\text{out}}$ projects to vocabulary.

\textbf{Capacity correspondence:} Model capacity corresponds to $\log_2|S_n|$, which should scale with:
\begin{itemize}
\item $\log_2 P$: Parameter count determines weight space dimension
\item $\log_2 V$: Vocabulary size determines output space dimension
\item Architecture factors: Depth, width, attention heads contribute multiplicatively
\end{itemize}

Thus each LLM instantiates $M_n$ at some hierarchy level $n_L$.

\begin{remark}[On the finite state machine abstraction]
As noted in Limitation~\ref{lim:simplified}, this mapping abstracts continuous neural network dynamics into discrete state transitions. While gradient descent operates in continuous parameter space and activations are real-valued, the effective computational behavior---especially at inference time---can be approximated by finite precision arithmetic. Modern GPUs use FP16 or INT8 quantization, making the finite state abstraction more realistic than it initially appears. However, the mapping remains approximate and experimental validation is essential.
\end{remark}

\subsection{Adjoint operations in practice}

\textbf{Projection (Distillation):} 
Teacher model $M_j$ distills to student $M_i$ ($i < j$) by minimizing KL divergence:
\[
L_{\text{distill}} = \mathbb{E}_x[\text{KL}(P_{\text{teacher}}(\cdot|x) \| P_{\text{student}}(\cdot|x))]
\]

This compresses high-entropy teacher outputs into lower-entropy student approximations. Information loss is:
\[
\Delta H = H(P_{\text{teacher}}) - H(P_{\text{student}})
\]

\textbf{Collapse (Fine-tuning):}
Starting from pre-trained model $M_i$, fine-tuning on domain-specific data enriches representations toward effective level $M_j$ ($j > i$). Formally:
\[
L_{\text{finetune}} = \mathbb{E}_{(x,y)\sim\mathcal{D}_{\text{target}}}[\text{CE}(y, M_i(x))]
\]

where $\mathcal{D}_{\text{target}}$ is the target domain dataset and CE is cross-entropy loss.

\textbf{Adjunction property:}
If distillation from $M_j$ to $M_i$ yields $\hat{M}_i$ and subsequent fine-tuning recovers $\hat{M}_j$, then:
\[
P_{j\to i}(C_{i\to j}(\hat{M}_i)) \approx \hat{M}_i \quad \text{(near-identity under projection-collapse)}
\]

This is the discrete analog of the counit $\varepsilon: P\circ C \Rightarrow \text{id}$.

\begin{remark}[Approximate vs.\ exact adjunction]
In practice, the adjunction holds only approximately due to optimization error, data distribution shift, and architectural constraints. The theoretical framework provides an idealized target; empirical validation should quantify deviation from exact adjunction.
\end{remark}

\section{Level Assignment and Validation}
\label{sec:level}

\subsection{Deriving the level formula}

\begin{proposition}[Level assignment formula]
\label{prop:level}
The hierarchy level $n_L$ of an LLM with $P$ parameters and vocabulary size $V$ is:
\[
n_L = \lfloor \alpha \log_2 P + \beta \log_2 V + \gamma \rfloor
\]
where $\alpha, \beta, \gamma$ are empirically determined constants.
\end{proposition}

\begin{proof}[Derivation]
Effective state space size $|S_n|$ scales with:
\begin{itemize}
\item Parameter count: Each parameter can take $\approx 2^b$ values (for $b$-bit quantization), giving $P^{2^b}$ configurations. In log-space: $\log_2|S_n| \approx b \cdot \log_2 P$.
\item Vocabulary: Output space is $V^{\text{context}}$ for contexts of bounded length, contributing $\log_2 V$ to capacity.
\item Architectural factors: Depth $L$, width $W$, attention heads $H$ contribute multiplicatively, absorbed into $\gamma$.
\end{itemize}

Setting $\alpha \approx b$ (quantization bits), $\beta \approx 1$ (first-order vocab contribution), and $\gamma$ as an architectural offset yields the formula. Empirical fitting to real models determines exact values.
\end{proof}

\begin{remark}[Expected coefficient values]
Based on typical neural network architectures:
\begin{itemize}
\item $\alpha \approx 0.5$--1.0 (reflecting effective parameter precision)
\item $\beta \approx 0.3$--0.7 (vocabulary contributes less than parameters)
\item $\gamma \approx 10$--20 (baseline architectural complexity)
\end{itemize}
These should be fitted empirically using benchmark performance data.
\end{remark}

\subsection{Validation methodology}

To validate the level assignment:

\begin{enumerate}
\item \textbf{Compute $n_L$} for 20--30 models across families (GPT, LLaMA, Falcon, Pythia, MPT)
\item \textbf{Measure performance} on benchmark tasks (GSM8K, Big-Bench Hard, HumanEval, GLUE)
\item \textbf{Fit coefficients} $(\alpha, \beta, \gamma)$ by regression: $\text{performance} \sim f(n_L)$
\item \textbf{Predict} performance for held-out models and compare to actual measurements
\item \textbf{Assess goodness of fit}: $R^2 > 0.85$ would indicate strong predictive power
\end{enumerate}

\section{Distillation as Projection: Thermodynamic Analysis}
\label{sec:distillation}

\subsection{Information-theoretic energy cost}

\begin{prediction}[Distillation energy scaling]
\label{pred:energy}
The energy required to distill teacher $M_j$ to student $M_i$ should scale as:
\[
E_{\text{distill}} = k_B T \cdot \Delta H + E_{\text{arch}}
\]
where:
\begin{itemize}
\item $k_B T$ is thermal energy ($T \approx 300$K for GPU operation)
\item $\Delta H = H(P_{\text{teacher}}) - H(P_{\text{student}})$ is information loss in bits
\item $E_{\text{arch}}$ is architecture-dependent computational overhead
\end{itemize}
\end{prediction}

\begin{remark}[Landauer's principle connection]
This prediction extends Landauer's principle (minimum energy $k_B T \ln 2$ per bit erased) to neural network compression. Each bit of entropy lost in distillation requires thermodynamic work. However, GPU energy consumption includes many non-thermodynamic factors (see Limitation~\ref{lim:physical}), so the relationship is expected to hold only as a baseline with substantial overhead.
\end{remark}

\subsection{Experimental protocol}

To test Prediction~\ref{pred:energy}:

\begin{enumerate}
\item \textbf{Select teacher-student pairs} spanning wide $\Delta H$ range (0.5--5 bits)
\item \textbf{Measure distillation energy} using GPU power monitoring (detailed in Appendix A.3 of the original manuscript)
\item \textbf{Quantify $\Delta H$} via entropy difference on validation set
\item \textbf{Fit linear model}: $E = a \cdot \Delta H + b$ and compare $a$ to $k_B T \ln 2 \approx 4.3 \times 10^{-21}$ J
\item \textbf{Interpret discrepancy}: Large $E_{\text{arch}}$ indicates overhead dominates; proportionality still validates information-theoretic scaling
\end{enumerate}

\begin{remark}[Confounding factors]
\label{rem:confound}
GPU energy consumption depends on:
\begin{itemize}
\item Memory bandwidth and cache efficiency
\item Parallelization overhead and synchronization
\item Floating-point operation throughput
\item Cooling and power regulation circuits
\end{itemize}
These factors can dominate the thermodynamic minimum. The prediction should be interpreted as: \emph{after controlling for architecture and hardware, energy should scale proportionally with information loss}. This requires careful experimental design with controlled hardware and multiple model families.
\end{remark}

\section{Scaling Predictions and Critical Transitions}
\label{sec:predictions}

\subsection{Emergence thresholds}

\begin{prediction}[Critical transition levels]
\label{pred:emergence}
Emergent abilities should appear near hierarchy levels $n \approx 33$--35, corresponding to models with $\approx 6$B--13B parameters (assuming typical vocabulary $V \approx 50$K and $\alpha \approx 0.8, \beta \approx 0.5, \gamma \approx 15$).
\end{prediction}

\textbf{Rationale:} If capabilities require representational capacity exceeding $2^{33} \approx 8.6 \times 10^9$ effective states, smaller models cannot implement the necessary computational circuits. The discontinuity arises from discrete level structure.

\begin{remark}[Falsifiability]
Prediction~\ref{pred:emergence} is highly falsifiable. If emergent abilities appear uniformly across all model sizes or at substantially different thresholds ($n < 30$ or $n > 38$), the hierarchical framework would require revision. Schaeffer et al.\ (2023) suggest some emergence may be measurement artifacts; distinguishing genuine phase transitions from smooth capability growth with nonlinear metrics is crucial.
\end{remark}

\subsection{Compression-performance tradeoff}

\begin{prediction}[Behavioral distance and performance degradation]
\label{pred:degradation}
Performance degradation under distillation should correlate with behavioral distance:
\[
\Delta \text{Acc} \propto \text{Beh}(n_{\text{teacher}}, n_{\text{student}}) + \Delta H
\]
where $\Delta \text{Acc}$ is accuracy loss on benchmark tasks.
\end{prediction}

\textbf{Validation:} Measure performance before/after distillation across model pairs, compute behavioral distance via output distribution divergence (KL divergence normalized by entropy), and fit regression model.

\subsection{Optimal distillation ratios}

The framework predicts diminishing returns when compressing beyond certain ratios:

\begin{corollary}[Distillation efficiency bound]
For teacher at level $n_j$ and student at level $n_i$, distillation efficiency $\eta = \text{Acc}_{\text{student}} / \text{Acc}_{\text{teacher}}$ should satisfy:
\[
\eta \geq \exp(-C \cdot (n_j - n_i))
\]
for some constant $C > 0$. This gives exponential performance decay with level gap.
\end{corollary}

\section{Discussion}
\label{sec:discussion}

\subsection{Theoretical limitations}

\subsubsection{Finite state abstraction}

As acknowledged in Limitation~\ref{lim:simplified}, treating neural networks as finite state machines ignores:
\begin{itemize}
\item \textbf{Continuous activations:} Real-valued hidden states have uncountably infinite precision (before quantization)
\item \textbf{Gradient dynamics:} Training involves continuous optimization in parameter space
\item \textbf{Stochasticity:} Sampling introduces randomness not captured by deterministic transitions
\end{itemize}

However, at inference time with fixed weights and finite precision arithmetic (FP16, INT8), the effective behavior is approximately finite-state. The degree of approximation quality is an empirical question.

\subsubsection{Thermodynamic predictions}

As noted in Limitation~\ref{lim:physical} and Remark~\ref{rem:confound}, GPU energy consumption has many confounding factors. The $E \propto k_B T \Delta H$ relationship should be viewed as a theoretical baseline. Empirical validation may reveal:
\begin{itemize}
\item Large architecture-dependent overhead $E_{\text{arch}} \gg k_B T \Delta H$
\item Non-trivial dependence on batch size, sequence length, and parallelization strategy
\item Hardware-specific effects (memory hierarchy, interconnect bandwidth)
\end{itemize}

The prediction remains valuable if energy scales \emph{proportionally} with $\Delta H$ after controlling for these factors, even if the absolute magnitude differs from theoretical minimum.

\subsection{Relation to existing theories}

\subsubsection{Neural scaling laws}

Our framework complements rather than replaces empirical scaling laws. Kaplan et al.\ (2020) and Hoffmann et al.\ (2022) provide phenomenological power-law fits. We offer a structural explanation: hierarchy levels impose discrete capacity thresholds, and the smooth average conceals stepwise transitions.

\subsubsection{Neural tangent kernel theory}

NTK theory analyzes overparameterized networks in the infinite-width limit where training becomes linear. Our finite-hierarchy framework applies to practical-sized models where discrete resource constraints matter. The two perspectives address different regimes.

\subsubsection{Information bottleneck}

Tishby \& Zaslavsky (2015) analyze layer-level compression-relevance tradeoffs. We extend this to model-level hierarchies across different architectures. The adjunction structure provides mathematical formalism for compression/expansion operations.

\subsection{Future work}

\subsubsection{Empirical validation}

Priority experiments:
\begin{enumerate}
\item Fit level assignment formula to 30+ models and validate on held-out test set
\item Measure distillation energy for 10+ teacher-student pairs across different $\Delta H$ ranges
\item Map emergence thresholds for specific capabilities (arithmetic, reasoning, code generation)
\item Quantify behavioral distance via output distribution divergence and correlate with performance
\end{enumerate}

\subsubsection{Theoretical extensions}

\begin{itemize}
\item \textbf{Continuous limit:} Replace discrete hierarchy with differential equations governing level transitions
\item \textbf{Multi-modal models:} Extend framework to vision-language models with cross-modal projections
\item \textbf{Training dynamics:} Incorporate gradient descent as trajectory through hierarchy levels
\item \textbf{Formal expressivity bounds:} Prove rigorous capacity limits at each hierarchy level
\end{itemize}

\subsubsection{Practical applications}

\begin{itemize}
\item \textbf{Compression algorithms:} Design distillation protocols minimizing $\text{Beh}(n_T, n_S)$
\item \textbf{Architecture search:} Optimize network design for target hierarchy level
\item \textbf{Capability prediction:} Estimate minimum model size for specific tasks from information-theoretic bounds
\end{itemize}

\section{Conclusion}
\label{sec:conclusion}

We have presented a theoretical framework situating large language models within a formal computational hierarchy. The key insights:

\begin{enumerate}
\item \textbf{Structural mapping:} LLMs instantiate finite machines at hierarchy levels determined by parameter count and vocabulary size

\item \textbf{Adjoint operations:} Distillation and fine-tuning realize projection/collapse operators with information-theoretic constraints

\item \textbf{Testable predictions:} Energy scaling, emergence thresholds, and compression-performance tradeoffs provide empirical falsification criteria

\item \textbf{Theoretical unification:} The framework connects scaling laws, information theory, and thermodynamics through categorical structure
\end{enumerate}

The framework's value lies not in immediate empirical confirmation but in providing \emph{precise quantitative predictions} that can be systematically tested. The specific threshold $n \approx 33$--35 for emergence, the energy-entropy relationship $E \propto k_B T \Delta H$, and the behavioral distance-performance correlation are all falsifiable hypotheses.

Limitations are substantial: the finite-state abstraction is approximate, thermodynamic predictions require careful interpretation, and empirical validation is entirely absent. However, these limitations are clearly stated, and the framework provides sufficient mathematical structure to guide experimental design.

If validated, this approach could transform our understanding of model scaling from phenomenological observation to principled theory grounded in computation and information. If falsified, the systematic exploration will reveal which aspects of neural networks defy hierarchical description---equally valuable for scientific progress.

\begin{thebibliography}{10}

\bibitem{ganguli2022}
Ganguli, D., et al. (2022).
\newblock Predictability and surprise in large generative models.
\newblock In \emph{Proceedings of FAccT}.

\bibitem{hinton2015}
Hinton, G., Vinyals, O., \& Dean, J. (2015).
\newblock Distilling the knowledge in a neural network.
\newblock \emph{arXiv preprint arXiv:1503.02531}.

\bibitem{hoffmann2022}
Hoffmann, J., et al. (2022).
\newblock Training compute-optimal large language models.
\newblock \emph{arXiv preprint arXiv:2203.15556}.

\bibitem{jiao2020}
Jiao, X., et al. (2020).
\newblock TinyBERT: Distilling BERT for natural language understanding.
\newblock In \emph{Proceedings of EMNLP}.

\bibitem{kaplan2020}
Kaplan, J., et al. (2020).
\newblock Scaling laws for neural language models.
\newblock \emph{arXiv preprint arXiv:2001.08361}.

\bibitem{kowalczyk2025}
Kowalczyk, K. (2025).
\newblock Adjoint projections on computational hierarchies: A metric framework.
\newblock \emph{Manuscript in preparation}.

\bibitem{sanh2019}
Sanh, V., et al. (2019).
\newblock DistilBERT, a distilled version of BERT: Smaller, faster, cheaper and lighter.
\newblock \emph{arXiv preprint arXiv:1910.01108}.

\bibitem{saxe2019}
Saxe, A. M., et al. (2019).
\newblock On the information bottleneck theory of deep learning.
\newblock \emph{Journal of Statistical Mechanics: Theory and Experiment}, 2019(12), 124020.

\bibitem{schaeffer2023}
Schaeffer, R., Miranda, B., \& Koyejo, S. (2023).
\newblock Are emergent abilities of large language models a mirage?
\newblock \emph{arXiv preprint arXiv:2304.15004}.

\bibitem{tishby2015}
Tishby, N., \& Zaslavsky, N. (2015).
\newblock Deep learning and the information bottleneck principle.
\newblock In \emph{Proceedings of ITW}.

\bibitem{tyszkiewicz1998}
Tyszkiewicz, J., \& Vianu, V. (1998).
\newblock Queries and computation on the web.
\newblock In \emph{Proceedings of ICDT}, 275--289.

\bibitem{tyszkiewicz2004}
Tyszkiewicz, J. (2004).
\newblock On asymptotic probabilities of monadic second order properties.
\newblock In \emph{Proceedings of ICALP}, 887--899.

\bibitem{tyszkiewicz2010}
Tyszkiewicz, J. (2010).
\newblock Kolmogorov complexity and expressive power.
\newblock \emph{Information and Computation}, 208(7), 729--743.

\bibitem{wei2022}
Wei, J., et al. (2022).
\newblock Emergent abilities of large language models.
\newblock \emph{Transactions on Machine Learning Research}.

\end{thebibliography}

\end{document}
